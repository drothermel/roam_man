{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96a12855-6fb2-4919-b5ea-b13de9d13d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dr_util.file_utils as fu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afbd5a0c-a937-4827-8073-dd6b30d1f8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "roam_data = fu.load_file(\"/Users/daniellerothermel/Desktop/drotherm_roam.json\")\n",
    "len(roam_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e10b179-4319-4194-a7f4-000974f77561",
   "metadata": {},
   "source": [
    "**Goal:** Programatically extract, surface and annotate useful things in work roam graph data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9799379-d5ef-4287-bee6-ce268a563268",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Define Clases & Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "92fbb012-dcdf-4ba2-ba5a-4e06352ed12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoamNode:\n",
    "    def __init__(self, json, start_depth=0):\n",
    "        self.refs_uid_blacklist = set([\n",
    "            'KVGudD7AP', # [[DONE]]\n",
    "            'e2rS3SVH7', # [[TODO]]\n",
    "        ])\n",
    "        if json is None:\n",
    "            raise Exception(\"None json page\")\n",
    "        if not isinstance(json, dict):\n",
    "            raise Exception(\n",
    "                \"RoamNode expects single page json\"\n",
    "            )\n",
    "            \n",
    "        self.raw_data = None\n",
    "        self.depth = start_depth\n",
    "\n",
    "        self.title = None\n",
    "        self.string = None\n",
    "        self.uid = None\n",
    "        self.create_time = None\n",
    "        self.edit_time = None\n",
    "        self.children = []\n",
    "        self.refs = []\n",
    "        self.other_keys = {}\n",
    "        \n",
    "        self.import_json(json)\n",
    "\n",
    "    def __repr__(self):\n",
    "        indent = \"  \" * self.depth\n",
    "        if self.title is not None:\n",
    "            rep_str = f\"{self.title}\\n\"\n",
    "            rep_str += f\" {indent} {self.uid=} {self.refs=}\\n\"\n",
    "        else:\n",
    "            rep_str = f\"{indent} - {self.string}\"\n",
    "            if len(self.refs) > 0:\n",
    "                rep_str += f\"\\n{indent} ==> {self.uid=} {self.refs=}\\n\"\n",
    "        #rep_str += (\n",
    "        #    f\" {indent} - {self.depth=}\\n\"\n",
    "        #    f\" {indent} - {self.uid=}\\n\"\n",
    "        #    f\" {indent} - {self.create_time=}\\n\"\n",
    "        #    f\" {indent} - {self.edit_time=}\\n\"\n",
    "        #    f\" {indent} - {len(self.children)=}\\n\"\n",
    "        #    f\" {indent} - {self.refs=}\\n\"\n",
    "        #    f\" {indent} - {self.other_keys.keys()}\\n\"\n",
    "        #)\n",
    "        return rep_str\n",
    "\n",
    "    def import_json(self, json):\n",
    "        self.raw_data =  json\n",
    "\n",
    "        for k, v in json.items():\n",
    "            if k[0] == ':' or \"user\" in k:\n",
    "                continue\n",
    "\n",
    "            # TODO: cleanup\n",
    "            if k == 'title':\n",
    "                self.title = v\n",
    "            elif k == 'string':\n",
    "                self.string = v\n",
    "            elif k == 'uid':\n",
    "                self.uid = v\n",
    "            elif k == 'create-time':\n",
    "                self.create_time = v\n",
    "            elif k == 'edit-time':\n",
    "                self.edit_time = v\n",
    "            elif k == 'children':\n",
    "                for ch in v:\n",
    "                    self.children.append(\n",
    "                        RoamNode(ch, start_depth=self.depth+1)\n",
    "                    )\n",
    "            elif k == 'refs':\n",
    "                for vv in v:\n",
    "                    if 'uid' in vv and vv['uid'] not in self.refs_uid_blacklist:\n",
    "                        self.refs.append(vv)\n",
    "            else:\n",
    "                self.other_keys[k] = v\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "962608bb-62a8-4e9a-8820-ba5b9ad19a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_roam_node(rn):\n",
    "    node_i = 0\n",
    "    nodes = [rn]\n",
    "    while node_i < len(nodes):\n",
    "        print(nodes[node_i])\n",
    "        for ch in nodes[node_i].children:\n",
    "            nodes.append(ch)\n",
    "        node_i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9206fb54-e324-4835-b5b1-438f4cfe3051",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecba5a9-855f-4ee1-83c0-be155da95ab4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Example of Daily Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "3130788f-b0b4-42ec-8523-79d2f5f82341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "October 8th, 2020\n",
      "  self.uid='10-08-2020' self.refs=[]\n",
      "\n",
      "   - **Thursday**\n",
      "   - Today I\n",
      "     - Made my RR account\n",
      "     - Watched: Attention is All You Need\n",
      "     - Setup: Ramping up on LMs note\n",
      "     - Watched the first two videos (intro to transfer learning and ðŸ¤—, lstm is dead)\n"
     ]
    }
   ],
   "source": [
    "print_roam_node(RoamNode(roam_data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6547cfac-3148-48c8-bd30-794abb15a642",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Example of New Paper Page Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "93dfbd94-3204-4dfc-b1ee-3f9f77bebb15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019| Solving Rubik's Cube with a Robot Hand\n",
      "  self.uid='Jb7PatuK4' self.refs=[]\n",
      "\n",
      "   - Paper: [arxiv](https://arxiv.org/abs/1910.07113)\n",
      "   - Authors: #OpenAI\n",
      "   ==> self.uid='RVlI9Mtgs' self.refs=[{'uid': '3zYNzVFy8'}]\n",
      "\n",
      "   - #[[Domain Randomization]] #[[Curriculum Learning]] #Sim2Real #[[Transfer Learning]] #[[[[Generalization]] in [[RL]]]]\n",
      "   ==> self.uid='l1UBICZk9' self.refs=[{'uid': 'q59cNnGpW'}, {'uid': '_4e_SPqLS'}, {'uid': 'Gnb9U6cJl'}, {'uid': '1_QMw77kQ'}, {'uid': 'R5SeictTQ'}, {'uid': 'eaCGvDycx'}, {'uid': 'yNs5rNiHf'}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_roam_node(RoamNode(roam_data[2000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96574d6f-df82-4f33-b6c7-b1331c29426d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Example of Old Paper Page Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "676f925f-1af1-46f8-9512-b63feb1ee98f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Is All You Need\n",
      "  self.uid='PshQdKxeS' self.refs=[]\n",
      "\n",
      "   - **Authors**: #[[Ashish Vaswani]], #[[Noam Shazeer]], #[[Jakob Uszkoreit]], #[[Llion Jones]], #[[Aidan N. Gomez]], #[[Lukasz Kaiser]], #[[Illia Polosukhin]]\n",
      "**Organization**: #Google #Papers2017\n",
      "**Topics**: #paperssorted #ToAnki #Transformers #NLP\n",
      "**Paper Link**: https://arxiv.org/abs/1706.03762\n",
      "**Video Link**: https://www.youtube.com/watch?v=iDulhoQ2pro&t=11s\n",
      "   ==> self.uid='CfOF1srRm' self.refs=[{'uid': 'LKqQsFmuk'}, {'uid': '3qZcbQeaM'}, {'uid': '0LcSf-pj6'}, {'uid': 'U8xh6WGvP'}, {'uid': 'wUGVJUtK8'}, {'uid': 'h8FxKBCFc'}, {'uid': 'fq2sVAtYR'}, {'uid': 'zUEH7Q5At'}, {'uid': 'a43hzItKe'}, {'uid': 'yrANQ6m5o'}, {'uid': 'sWHx_W9Xx'}, {'uid': 'LN_hZEpob'}, {'uid': 'oVmuQcd4q'}]\n",
      "\n",
      "   - ----------------------------------\n",
      "   - Processing Stages\n",
      "   - ----------------------------------\n",
      "   - **Summary**\n",
      "   - **Synthesized Detailed Notes**\n",
      "     - {{[[DONE]]}} Easy Notes\n",
      "     - {{[[DONE]]}} Synthesize Easy Notes\n",
      "     - {{[[DONE]]}} Paper Pass\n",
      "     - {{[[DONE]]}} Synthesize All Notes\n",
      "     - {{[[TODO]]}} Summary\n",
      "     - {{[[TODO]]}} Anki\n",
      "     - Introduction\n",
      "     - Related Work\n",
      "     - Model architecture\n",
      "     - Training method\n",
      "     - Discussion\n",
      "       - This paper introduces the transformer, an encoder decoder architecture that uses self-attention layers to turn input sequences into results.\n",
      "       - A great summary of this can be found here: [[The Illustrated Transformer]]\n",
      "       ==> self.uid='JUdPM0qOa' self.refs=[{'uid': 'faSppflh1'}]\n",
      "\n",
      "       - [[LSTM]]s - the encoder goes over the source sentence one token at a time producing a hidden state for each.  The last hidden state is then passed to the decoder which then iterates through producing output tokes (and new hidden states) until the output sequence is complete.\n",
      "       ==> self.uid='PRROhYuAR' self.refs=[{'uid': '-VuCrUyTc'}]\n",
      "\n",
      "       - Input/Output Embedding - done the same for both\n",
      "       - Encoder - takes the full input sequence and produces values and keys for each of the input tokens. Uses 6 [[Self Attention]] layers (each with different weights).\n",
      "       ==> self.uid='oRpxkxQaw' self.refs=[{'uid': 'hypS1sgeo'}]\n",
      "\n",
      "       - Decoder - two parts:\n",
      "       - Feed forward layer + task specific head.\n",
      "       - Other notes\n",
      "       - Every step of decoding is one training sample (because you can backprop) unlike the RNN where you had to process the entire sequence before backpropping.\n",
      "       - Adam with a varied learning rate which increased linearly for the first warmup steps and then decreased thereafter proportionally to the inverse square root of the step number.\n",
      "       - They used label smoothing during training.\n",
      "       - The self-attention lowers the effective resolution due tot he weighted averaging over the values, but this is partially offset by the multi-headed nature. (??)\n",
      "       - All layers are the same size to facilitate residual connections.\n",
      "       - Performance notes:\n",
      "         - The distance between a slot in the input and the loosely correspond slot in the output is about the length of the input sequence, very far apart.\n",
      "         - Attention makes it so the distance between any point in the input sequence and any point in the output sequence is O(1).\n",
      "         - Embed the sequence into a vector by first tokenizing and then using a dictionary to produce the word vectors.\n",
      "         - Then concatenate the position encodings, a sequence of sin/cos waves with different wavelengths to allow the model to know where each token is spatially in the input.\n",
      "         - Apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks.\n",
      "         - Part one takes in what exists of the output sequence so far and embeds it, produces queries. Uses 6 [[Self Attention]] layers (each with different weights).\n",
      "         ==> self.uid='Ed3J3f9n0' self.refs=[{'uid': 'hypS1sgeo'}]\n",
      "\n",
      "         - Part two takes the keys and values from the encoder and the queries from part one and produces output. Uses one [[Self Attention]] layer.\n",
      "         ==> self.uid='7WAqP56KZ' self.refs=[{'uid': 'hypS1sgeo'}]\n",
      "\n",
      "         - Actually using multi-headed self attention, which means that there are multiple sets of self attention weights (so each can attend to different things).\n",
      "         - Dropout is applied to each sublayer before it is added to its input and then normalized with layernorm.\n",
      "         - Each self-attention block has residual connections.\n",
      "         - Also note that while the same feedforward NN is applied to each position in the sequence, each encoder/decoder block has its own feedforward NN.\n",
      "         - Same weight matrix between the two embedding layers and the pre-softmax linear transformation.\n",
      "         - 28.4 BLUE on WMT 2014 English to German\n",
      "         - 41.8 BLUE on WMT 2014 English to French\n"
     ]
    }
   ],
   "source": [
    "print_roam_node(RoamNode(roam_data[2]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
